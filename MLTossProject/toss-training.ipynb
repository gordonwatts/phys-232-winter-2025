{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toss Training\n",
    "\n",
    "We'll load in the data that people uploaded for HW and use that to train a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Pull most of this from our data examination notebook. However, the data we want to train on is a little different. \n",
    "\n",
    "We are going to analyze each _toss_. That means all the data from a single toss is going to be fed to our network in order to understand what that _toss_ was.\n",
    "\n",
    "Looking back at the previous training - that means each toss is going to be a _row_ in our training data. This means we need to load in the data from each toss, and _transpose_ it so that each measurement is a column.\n",
    "\n",
    "Finally - we'll look only at the _total_ acceleration, as we did in the previous plots.\n",
    "\n",
    "We saw that the number of measurements is 26-27 - so lets take only the first 25 to be safe. So we will have 25 inputs. We need to rotate that into a column, which we will then append to the master `DataFrame`. One row per file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the CSV files are located\n",
    "directory = Path('./data/ClassData')\n",
    "\n",
    "def fetch_data(sub_dir_name: str) -> pd.DataFrame:\n",
    "    # Define the directory where the CSV files are located\n",
    "    f_dir = directory / sub_dir_name\n",
    "    # Recursively get a list of all .txt files in this directory and below.\n",
    "    csv_files = list(f_dir.glob('**/*.txt'))\n",
    "    # Initialize an empty list to store the DataFrames\n",
    "    dfs = []\n",
    "    # Loop over the list of CSV files\n",
    "    for index, file in enumerate(csv_files):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_sample = pd.read_csv(file)\n",
    "        # Calculate the total acceleration\n",
    "        df_sample['a'] = (df_sample.ax**2 + df_sample.ay**2 + df_sample.az**2).apply(sqrt)\n",
    "        # Convert the 'a' column into a numpy array\n",
    "        a = df_sample['a'].to_numpy()\n",
    "        # Transpose it so that it is a single row with 25 columns.\n",
    "        a_col = a.reshape(1, -1)[0][:25]\n",
    "        # Create a dataframe with the 25 columns, labeled \"a1\", \"a2\", etc.\n",
    "        df_row = pd.DataFrame(a_col).T\n",
    "        df_row.columns = ['a' + str(i) for i in range(1, 26)]\n",
    "        dfs.append(df_row)\n",
    "    # Concatenate all the DataFrames in the list into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load in the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holding = fetch_data('held')\n",
    "df_horizontal = fetch_data('horizontal')\n",
    "df_up = fetch_data('up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to label it. This is the expected output. We have two choices for our NN output. First one, is a single number that goes from zero to 3 (say). The other choice is to have three outputs from the network. The first if the toss was directly `up`, the second `horizontal`, and the third `holding`. This three-output is the right way to go. Otherwise the network will try to interpolate between the three for a single output.\n",
    "\n",
    "Another way to think about this: what should the network do if the actual toss was very close between a toss straight up and a toss onto a couch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(df: pd.DataFrame, label_holding: bool, label_horizontal: bool, label_up: bool):\n",
    "    'In place add new columns to the DataFrame to store the label'\n",
    "    df['is_up'] = np.ones(len(df)) if label_up else np.zeros(len(df))\n",
    "    df['is_horizontal'] = np.ones(len(df)) if label_horizontal else np.zeros(len(df))\n",
    "    df['is_holding'] = np.ones(len(df)) if label_holding else np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_label(df_holding, True, False, False)\n",
    "add_label(df_horizontal, False, True, False)\n",
    "add_label(df_up, False, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training data is everything all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_holding, df_horizontal, df_up], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training\n",
    "\n",
    "Create training and test samples for later work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, :-3]\n",
    "y = data.iloc[:, -3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those numbers are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets copy the network from the sample thing we did earlier. But... we then change things:\n",
    "\n",
    "* We have 25 inputs\n",
    "* We have 3 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(25, activation='relu', input_shape=(25,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the results!\n",
    "\n",
    "For each output, we want to look at it for each class of toss. For holding the bluefruit, we'd expect the _up_ and _horizontal_ to both be zeros, and the _holding_ to be one. Lets plot all this.\n",
    "\n",
    "First, of course, we need to run the prediction on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict(x_test)\n",
    "y_predict = pd.DataFrame(y_p, columns=['p_up', 'p_horizontal', 'p_holding'])\n",
    "y_predict['is_up'] = y_test['is_up'].to_numpy()\n",
    "y_predict['is_horizontal'] = y_test['is_horizontal'].to_numpy()\n",
    "y_predict['is_holding'] = y_test['is_holding'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filter in ['is_up', 'is_horizontal', 'is_holding']:\n",
    "    for col in ['p_up', 'p_horizontal', 'p_holding']:\n",
    "        plt.hist(y_predict[y_predict[filter] == 1.0][col], alpha=0.5, label='up', range=(0,1))\n",
    "        plt.xlabel(col)\n",
    "        plt.title(f\"{col} for {filter}=1.0\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
